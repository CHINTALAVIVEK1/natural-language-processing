{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/zu19eEpndv2W1eWjTG3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHINTALAVIVEK1/natural-language-processing/blob/main/nlp_task11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuuhW4TxoByl",
        "outputId": "74932f00-d422-4957-dc6a-79b2660ad720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Chunks:\n",
            "This website is using a security service to protect itself from online attacks\n",
            "The action you just performed triggered the security solution\n",
            "There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data\n",
            "You can email the site owner to let them know you were blocked\n",
            "Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page\n"
          ]
        }
      ],
      "source": [
        "import re, requests, torch\n",
        "import torch.nn as nn\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "class ChunkerModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.lstm, self.fc = nn.LSTM(128, 64, batch_first=True), nn.Linear(64, 1)\n",
        "    def forward(self, x): return torch.sigmoid(self.fc(self.lstm(x)[0][:, -1, :]))\n",
        "def fetch_text(url):\n",
        "    return re.sub(r'\\s+', ' ', ' '.join(p.get_text() for p in BeautifulSoup(requests.get(url).text,\n",
        "    'html.parser').find_all('p')))\n",
        "def preprocess(text):\n",
        "    tok = Tokenizer(num_words=5000)\n",
        "    tok.fit_on_texts([text])\n",
        "    sequences = tok.texts_to_sequences([text])\n",
        "    # Check if sequences is empty and return an empty padded sequence if it is\n",
        "    if not sequences or not sequences[0]:\n",
        "      return pad_sequences([[]], maxlen=100, padding='post'), tok\n",
        "    return pad_sequences(sequences, maxlen=100, padding='post'), tok\n",
        "def segment_text(url):\n",
        "    text = fetch_text(url)\n",
        "    if not text: # Check if text is empty\n",
        "        return []\n",
        "    # Split the text and return the first 5 chunks if they exist\n",
        "    chunks = text.split('. ')\n",
        "    return chunks[:5] if chunks and chunks[0] else []\n",
        "print(\"Extracted Chunks:\",*segment_text(\"https://dictionary.cambridge.org/dictionary/english/content\"), sep=\"\\n\")"
      ]
    }
  ]
}