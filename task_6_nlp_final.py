# -*- coding: utf-8 -*-
"""task 6 nlp final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFXtSpU6HR3DXnDUUcwhF-pk1q5hg0n3
"""

import nltk
nltk.download('punkt_tab')

from nltk.util import ngrams
from nltk.lm import Laplace
from nltk.tokenize import word_tokenize
from nltk.lm.preprocessing import padded_everygram_pipeline

def ngram_smoothing(sentence, n):
    tokens = word_tokenize(sentence.lower())
    train_data, padded_sents = padded_everygram_pipeline(n, tokens)
    model = Laplace(n)
    model.fit(train_data, padded_sents)
    return model

sentence = input("Enter a sentence: ")
n = int(input("Enter the value of N for N-grams: "))

model = ngram_smoothing(sentence, n)
context_words = sentence.lower().split()
context = tuple(context_words[max(0, len(context_words) - n + 1):])

if len(context) < n - 1:
    context = (None,) * (n - 1 - len(context)) + context
next_words = model.generate(3, text_seed=context)
print("Next words:", ' '.join(next_words))